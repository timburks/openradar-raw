apiVersion: openapi/v1alpha1
kind: Radar
metadata:
    name: "21540065"
    labels:
        datastore_id: "5006714176798720"
data:
    classification: Enhancement
    created: "2015-06-25T05:07:56.49829Z"
    description: |-
        watchOS 2.0 introduced the concept of semantic haptics with the `-[WKInterfaceDevice playHaptic:]` API. I would really like to use this on iOS. Being able to have a consistent set of haptic & audio queues to use for well-defined UX events would bring a level of polish and consistency to iOS apps that is difficult to achieve today. As a contractor, most large companies aren’t interested in investing in sound design for their apps; as an independent developer I simply don’t have the resources for recording and implementing professional-quality sound design. As both, the only public option I have for device vibration is the somewhat clumsy `AudioServicesPlaySystemSound (kSystemSoundID_Vibrate)` API. Having the ability to simply call `playHaptic:` at the appropriate times would be great.

        This could also be an accessibility boon, as it would provide a common vocabulary for non-visual UX feedback. I think it would be a great augmentation for a VoiceOver interface that—while powerful—is relatively one dimensional.
    email: frozendevil@gmail.com
    modified: "2015-06-25T05:07:56.49849Z"
    number: "21540065"
    number_intvalue: 21540065
    originated: 24-Jun-2015 10:07 PM
    parent_number: '&{NULL_VALUE}'
    product: iOS SDK
    product_version: iOS 9.0
    reproducible: Not Applicable
    resolved: ""
    status: Open
    title: Semantic feedback/haptics on iOS
